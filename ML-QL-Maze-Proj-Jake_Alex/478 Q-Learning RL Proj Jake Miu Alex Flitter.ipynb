{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23392609",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cbcc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pygame used for rendering our maze environment & agent's actions\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3129c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stable-baselines is where we pull the Deep Q-Network from\n",
    "!pip install stable-baselines3[extra] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN = Deep-Q-Network - maximize bellman equation, MLP Policy for model\n",
    "import gym \n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820232fa",
   "metadata": {},
   "source": [
    "# Specify the Rendering for the Pygame Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52386a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH, HEIGHT = 800, 800\n",
    "#creates the window\n",
    "WIN = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "#Gives the window a name in the top left corner\n",
    "pygame.display.set_caption(\"Maze Render!\")\n",
    "#RGB color constants\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (50, 205, 50)\n",
    "BLUE = (176,224,230)\n",
    "#How many frames per second we want to show. When we are accually testing this is should be around 20-45\n",
    "FPS = 5000\n",
    "#Background color(currently does nothing because default is white)\n",
    "WIN.fill(WHITE)\n",
    "# notify user the display should pop up in a video device, if not rendering code not needed\n",
    "print(\"Video display should exist in pop up window as of now\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The goal of this funtion is to draw the walls of the maze in the correct areas\n",
    "#The for loop stuff should only have to be done once, it is currently being done more than one time(This is fine, but unneeded)\n",
    "def draw_window(state, past_state):#past_past_state \n",
    "    #Go through all indexes of WALLS\n",
    "    for x in range(2*DIM_COL_ROW + 1):\n",
    "        for y in range(DIM_COL_ROW + 1):\n",
    "            #If there should be a wall there\n",
    "            if WALLS[x][y] == 1:\n",
    "                #Check to see if the line is vertical\n",
    "                if x % 2 == 0:\n",
    "                    #pygame.draw.rect takes in 3 inputs (window it is drawing to, Color in RGB, (start location of x, start location of y, how long in x direction, how long in y direction))\n",
    "                    pygame.draw.rect(WIN, BLACK, (100 + x/2 * 100, (6-y) * 100, 10, 110))\n",
    "                #Line is horizonal\n",
    "                else:\n",
    "                    pygame.draw.rect(WIN, BLACK, (100 + (x//2 * 100), 100 +(DIM_COL_ROW - y) * 100, 110, 10))\n",
    "    #pygame.draw.circle takes in 4 inputs (window it is drawing to, Color in RGB, center, radius)\n",
    "    #pygame.draw.circle(WIN, WHITE,(100 * past_past_state[0] + 155, 55 +(DIM_COL_ROW - past_past_state[1]) * 100), 25)\n",
    "    pygame.draw.circle(WIN, BLUE,(100 * past_state[0] + 155, 55 +(DIM_COL_ROW - past_state[1]) * 100), 25)\n",
    "    pygame.draw.circle(WIN, RED, (100 * state[0] + 155, 55 +(DIM_COL_ROW - state[1]) * 100), 25)\n",
    "    pygame.draw.circle(WIN, GREEN, (DIM_COL_ROW * 100 + 55, 155), 25)\n",
    "    #This takes everything that is on the draw stack and pushes it to the window. \n",
    "    pygame.display.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8dcc7f",
   "metadata": {},
   "source": [
    "# Build the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04089410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for maze env where agent will learn\n",
    "class MazeEnv(Env):\n",
    "    \n",
    "    # maze member_variables & action / observation space\n",
    "    def __init__(self, DIM_COL_ROW, STARTING_CELL, WALLS):\n",
    "        # declare num_col & num_row; boards are square\n",
    "        self.dim_row_col = DIM_COL_ROW\n",
    "        # declare starting cell\n",
    "        self.starting_cell = STARTING_CELL\n",
    "        # actions we can take, up, right, down, left ; NESW\n",
    "        self.action_space = Discrete(4) # 0,1,2,3\n",
    "        # observation - box w/ np.arrays made discrete elements\n",
    "        self.observation_space = Box(np.array((0,0), dtype=int), np.array((DIM_COL_ROW-1,DIM_COL_ROW-1)), dtype=np.int64)\n",
    "        # declare curr state to none & initialize in step function if empty\n",
    "        self.state = None\n",
    "        # state before the action is made\n",
    "        self.past_state = self.starting_cell\n",
    "        # set max_steps to prevent infinite searching in maze\n",
    "        self.max_steps = 100000 # 1000 baseline, will change\n",
    "        # set current step for iterating action steps\n",
    "        self.current_step = 0\n",
    "        # set value for cell in maze that ends episode\n",
    "        self.end_cell = np.array((DIM_COL_ROW-1,DIM_COL_ROW-1),dtype=np.int64)\n",
    "        # set episode termination variable to false\n",
    "        self.episode_terminated = False\n",
    "        # set walls for maze below\n",
    "        self.Walls = WALLS\n",
    "        \n",
    "    # moves agent around env; how actions change states\n",
    "    def step(self, action):\n",
    "        # assigns starting cell; done this way to avoid assertion errors\n",
    "        try:\n",
    "            _ = self.state[0]\n",
    "        except ValueError:\n",
    "            self.state = self.starting_cell\n",
    "        # reward definition for value mutations later on in if statements\n",
    "        # action discrete values defined below\n",
    "        # 0 is down \n",
    "        # 1 is left\n",
    "        # 2 is up\n",
    "        # 3 is right\n",
    "        # take action & change state cell; passes prevent action in else\n",
    "        # if action is down or up\n",
    "        if (action == 0 or action == 2):\n",
    "            # if wall DNE\n",
    "            if (self.Walls[2*self.state[0]+1, self.state[1]+ (action//2)] == 0):\n",
    "                #move past state\n",
    "                self.past_state = self.state\n",
    "                # move to new state\n",
    "                self.state = (self.state[0], self.state[1] + (action - 1))\n",
    "                # increment steps\n",
    "                self.current_step += 1    \n",
    "            # if wall exists\n",
    "            else:\n",
    "                pass\n",
    "        # else action is left or right\n",
    "        else:\n",
    "            # if wall DNE\n",
    "            if (self.Walls[2*(self.state[0] + (action//2)) , self.state[1]] == 0):\n",
    "                #move past state\n",
    "                self.past_state = self.state\n",
    "                # move to new state\n",
    "                self.state = (self.state[0] + (action - 2), self.state[1])\n",
    "                # increment steps\n",
    "                self.current_step += 1\n",
    "            # if wall exists\n",
    "            else:\n",
    "                pass \n",
    "        \n",
    "        # calculate reward & check if at end-condition\n",
    "        if (np.array_equal(self.state,self.end_cell)):    \n",
    "            self.episode_terminated = True\n",
    "            reward = 100\n",
    "        else:\n",
    "            # incentive to keep moving; reach end quickly\n",
    "            reward = -1/(self.dim_row_col*self.dim_row_col)\n",
    "            \n",
    "        # end-condition w/ out reward is too many steps taken\n",
    "        if (self.current_step >= self.max_steps):\n",
    "            self.episode_terminated = True\n",
    "            \n",
    "        # info {} must be returned for step() for Env class; dk why\n",
    "        info = {}\n",
    "        \n",
    "        # return step information\n",
    "        return np.array(self.state, dtype=np.int64), reward, self.episode_terminated, info\n",
    "                \n",
    "    # implements visuals of learning process\n",
    "    def render(self, mode):\n",
    "        # implement viz\n",
    "        clock = pygame.time.Clock()\n",
    "        clock.tick(FPS)\n",
    "        draw_window(self.state, self.past_state)#self.past_past_state\n",
    "        \n",
    "        for event in pygame.event.get():\n",
    "        #Are any of the inputs the X in the top left\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "    \n",
    "    # reset sets env's params to starting values\n",
    "    def reset(self):\n",
    "        # reset state to starting cell\n",
    "        self.state = np.array(self.starting_cell, dtype=np.int64)\n",
    "        # reset past state to starting cell\n",
    "        self.past_state = np.array(self.starting_cell, dtype=np.int64)\n",
    "        # reset episode_terminated to episode running\n",
    "        self.episode_terminated = False\n",
    "        # reset current step to no steps taken\n",
    "        self.current_step = 0\n",
    "        # reset the whole window\n",
    "        WIN.fill(WHITE)\n",
    "        # return state to exploit; model.predict(env.reset()) in test model\n",
    "        #return np.array(self.state, dtype=np.int64)\n",
    "        return self.state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dimensions of square maze & starting cell in maze\n",
    "DIM_COL_ROW = 6\n",
    "STARTING_CELL = (0,0)\n",
    "# walls is subject to dim; hardcode changes on input\n",
    "# even x indices are vert walls; up & down actions permittable if val = 0\n",
    "# odd x indices are horz walls; right and left actions permittable if val = 0\n",
    "# end-points of x indices are the edges of the grid\n",
    "# val = 2 => noise in wall grid (wall N/A; neither T/F)\n",
    "# specifically, val = 2 => vert & edges-grid top of grid\n",
    "# for Walls, each row is a set of edges-grid + vert + horz  borders\n",
    "# for Walls, num_col = dim_col_row+1; border numbers\n",
    "WALLS = np.array([\n",
    "                [1,1,1,1,1,1,2],\n",
    "                [1,0,0,0,0,0,1],\n",
    "                [1,0,1,0,1,0,2],\n",
    "                [1,1,1,1,0,1,1],\n",
    "                [0,0,0,0,0,0,2],\n",
    "                [1,0,1,1,1,1,1],\n",
    "                [1,1,0,0,1,0,2],\n",
    "                [1,0,0,0,0,0,1],\n",
    "                [1,1,1,0,1,1,2],\n",
    "                [1,0,0,0,1,1,1],\n",
    "                [0,1,1,1,0,0,2],\n",
    "                [1,0,0,0,0,0,1],\n",
    "                [1,1,1,1,1,1,2]\n",
    "                ])\n",
    "\n",
    "# declare environment\n",
    "env = MazeEnv(DIM_COL_ROW, STARTING_CELL, WALLS)\n",
    "# ensure env functions with stable_baselines well\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17607857",
   "metadata": {},
   "source": [
    "# Test The Environment- Random Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 20\n",
    "average_score = 0\n",
    "average_steps = 0\n",
    "# iterate through simulated episodes of the env with random actions\n",
    "for episode in range(1, episodes+1):\n",
    "    # make state at starting cell\n",
    "    state = env.reset()\n",
    "    # reset boolean for simulation\n",
    "    done = False\n",
    "    # score is metric for rewards\n",
    "    score = 0 \n",
    "    # logic for scores\n",
    "    # simulate a singular episode with random action\n",
    "    while not done:\n",
    "        # render env\n",
    "        env.render(True)\n",
    "        # random actions hence .sample()\n",
    "        action = env.action_space.sample()\n",
    "        # step from state with action & capture step info\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        # increment reward metric\n",
    "        score+=reward\n",
    "    # display simulation results\n",
    "    print('Episode:{} Score:{} Steps:{}'.format(episode, score, int((100-score)*(DIM_COL_ROW*DIM_COL_ROW))))\n",
    "    # update average_score & average_steps\n",
    "    average_score += score\n",
    "    average_steps += int((100-score)*(DIM_COL_ROW*DIM_COL_ROW))\n",
    "# display simulation average score & steps\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Average Score:{} Average Steps:{}\".format(average_score/episodes, average_steps/episodes))\n",
    "# env.close() - needed when render implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f08715",
   "metadata": {},
   "source": [
    "## Train an RL Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2b8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the paths for saving the model & logging its training statistics\n",
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "log_path = os.path.join('Training', 'Logs')\n",
    "training_log_path = os.path.join(log_path, 'DQN_7')\n",
    "dqn_path = os.path.join('Training', 'Saved Models', 'DQN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Please explicitly type Learn or Load to indicate if you'd like to Learn a new model or Load in the existing Learned model\")\n",
    "print(\"Note learning a model can take from 5 to 20 minutes. We HIGHLY suggest loading the model instead of re-learning.\")\n",
    "print(\"-- As this is a .ipynb file, search for where to input your answer if need be  --\")\n",
    "print(\"-- in vscode the input is at the top of your screen; jupyter notebooks the input is in the cell-block's output --\")\n",
    "userInput = input(\"Learn or Load: \")\n",
    "# user input validation\n",
    "while(userInput.lower() != \"learn\" and userInput.lower() != \"load\"):\n",
    "    userInput = input(\"Invalid entry. Please type learn or load to indicate your requested action: \")\n",
    "# execution to learn or load\n",
    "if userInput.lower() == \"learn\":\n",
    "    # create the model object with MlpPolicy & DQN type\n",
    "    model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "    # train the model\n",
    "    model.learn(total_timesteps=500000)\n",
    "    # save model to dqn path\n",
    "    model.save(dqn_path)\n",
    "    # display success\n",
    "    print(\"Successfully learned upon a new DQN model\")\n",
    "else:\n",
    "    # load model from dqn_path including the existing env\n",
    "    model = DQN.load(dqn_path, env=env)\n",
    "    # display success\n",
    "    print(\"Successfully loaded the prexisting DQN model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b534a",
   "metadata": {},
   "source": [
    "## Test DQN_Model on MazeEnv - See Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DQN Model's predicted path from starting state from {} to {} below\".format(STARTING_CELL, (DIM_COL_ROW, DIM_COL_ROW)))\n",
    "# reset env, run agent predictions until step function indicates ending cell reached\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    print(\"obs: \", obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render(True)\n",
    "    if done: \n",
    "        print('obs: ', obs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run policy on env for n_eval_episodes & return mean reward per episode\n",
    "evaluate_policy(model, env, n_eval_episodes=1000, render=True, reward_threshold = 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0562836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this line to quit popup without crashing kernel\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66bab98fb05b5ac4a4515b72f66cfb10345e282e1f09ccb74aec644d67d52634"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
