{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23392609",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73cbcc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in t:\\installs\\anaconda_location\\lib\\site-packages (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f341c988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# DQN = Deep-Q-Network - maximize bellman equation, MLP Policy for model\n",
    "import gym \n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "from random import randint\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8dcc7f",
   "metadata": {},
   "source": [
    "# Building the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a52386a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rect(0, 0, 800, 800)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WIDTH, HEIGHT = 800, 800\n",
    "#creates the window\n",
    "WIN = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "#Gives the window a name in the top left corner\n",
    "pygame.display.set_caption(\"Maze Render!\")\n",
    "#RGB color constants\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (50, 205, 50)\n",
    "BLUE = (176,224,230)\n",
    "#How many frames per second we want to show. When we are accually testing this is should be around 20-45\n",
    "FPS = 5000\n",
    "#Background color(currently does nothing because default is white)\n",
    "WIN.fill(WHITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d3a9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The goal of this funtion is to draw the walls of the maze in the correct areas\n",
    "#The for loop stuff should only have to be done once, it is currently being done more than one time(This is fine, but unneeded)\n",
    "def draw_window(state, past_state):#past_past_state \n",
    "    #Go through all indexes of WALLS\n",
    "    for x in range(2*DIM_COL_ROW + 1):\n",
    "        for y in range(DIM_COL_ROW + 1):\n",
    "            #If there should be a wall there\n",
    "            if WALLS[x][y] == 1:\n",
    "                #Check to see if the line is vertical\n",
    "                if x % 2 == 0:\n",
    "                    #pygame.draw.rect takes in 3 inputs (window it is drawing to, Color in RGB, (start location of x, start location of y, how long in x direction, how long in y direction))\n",
    "                    pygame.draw.rect(WIN, BLACK, (100 + x/2 * 100, (6-y) * 100, 10, 110))\n",
    "                #Line is horizonal\n",
    "                else:\n",
    "                    pygame.draw.rect(WIN, BLACK, (100 + (x//2 * 100), 100 +(DIM_COL_ROW - y) * 100, 110, 10))\n",
    "    #pygame.draw.circle takes in 4 inputs (window it is drawing to, Color in RGB, center, radius)\n",
    "    #pygame.draw.circle(WIN, WHITE,(100 * past_past_state[0] + 155, 55 +(DIM_COL_ROW - past_past_state[1]) * 100), 25)\n",
    "    pygame.draw.circle(WIN, BLUE,(100 * past_state[0] + 155, 55 +(DIM_COL_ROW - past_state[1]) * 100), 25)\n",
    "    pygame.draw.circle(WIN, RED, (100 * state[0] + 155, 55 +(DIM_COL_ROW - state[1]) * 100), 25)\n",
    "    pygame.draw.circle(WIN, GREEN, (DIM_COL_ROW * 100 + 55, 155), 25)\n",
    "    #This takes everything that is on the draw stack and pushes it to the window. \n",
    "    pygame.display.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04089410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for maze env where agent will learn\n",
    "class MazeEnv(Env):\n",
    "    \n",
    "    # maze member_variables & action / observation space\n",
    "    def __init__(self, DIM_COL_ROW, STARTING_CELL, WALLS):\n",
    "        # declare num_col & num_row; boards are square\n",
    "        self.dim_row_col = DIM_COL_ROW\n",
    "        # declare starting cell\n",
    "        self.starting_cell = STARTING_CELL\n",
    "        # actions we can take, up, right, down, left ; NESW\n",
    "        self.action_space = Discrete(4) # 0,1,2,3\n",
    "        # observation - box w/ np.arrays made discrete elements\n",
    "        self.observation_space = Box(np.array((0,0), dtype=int), np.array((DIM_COL_ROW-1,DIM_COL_ROW-1)), dtype=np.int64)\n",
    "        # declare curr state to none & initialize in step function if empty\n",
    "        self.state = None\n",
    "        # state before the action is made\n",
    "        self.past_state = self.starting_cell\n",
    "        # set max_steps to prevent infinite searching in maze\n",
    "        self.max_steps = 100000 # 1000 baseline, will change\n",
    "        # set current step for iterating action steps\n",
    "        self.current_step = 0\n",
    "        # set value for cell in maze that ends episode\n",
    "        self.end_cell = np.array((DIM_COL_ROW-1,DIM_COL_ROW-1),dtype=np.int64)\n",
    "        # set episode termination variable to false\n",
    "        self.episode_terminated = False\n",
    "        # set walls for maze below\n",
    "        self.Walls = WALLS\n",
    "        \n",
    "    # moves agent around env; how actions change states\n",
    "    def step(self, action):\n",
    "        # assigns starting cell; done this way to avoid assertion errors\n",
    "        try:\n",
    "            _ = self.state[0]\n",
    "        except ValueError:\n",
    "            self.state = self.starting_cell\n",
    "        # reward definition for value mutations later on in if statements\n",
    "        # action discrete values defined below\n",
    "        # 0 is down \n",
    "        # 1 is left\n",
    "        # 2 is up\n",
    "        # 3 is right\n",
    "        # take action & change state cell; passes prevent action in else\n",
    "        # if action is down or up\n",
    "        if (action == 0 or action == 2):\n",
    "            # if wall DNE\n",
    "            if (self.Walls[2*self.state[0]+1, self.state[1]+ (action//2)] == 0):\n",
    "                #move past state\n",
    "                self.past_state = self.state\n",
    "                # move to new state\n",
    "                self.state = (self.state[0], self.state[1] + (action - 1))\n",
    "                # increment steps\n",
    "                self.current_step += 1    \n",
    "            # if wall exists\n",
    "            else:\n",
    "                pass\n",
    "        # else action is left or right\n",
    "        else:\n",
    "            # if wall DNE\n",
    "            if (self.Walls[2*(self.state[0] + (action//2)) , self.state[1]] == 0):\n",
    "                #move past state\n",
    "                self.past_state = self.state\n",
    "                # move to new state\n",
    "                self.state = (self.state[0] + (action - 2), self.state[1])\n",
    "                # increment steps\n",
    "                self.current_step += 1\n",
    "            # if wall exists\n",
    "            else:\n",
    "                pass \n",
    "        \n",
    "        # calculate reward & check if at end-condition\n",
    "        if (np.array_equal(self.state,self.end_cell)):    \n",
    "            self.episode_terminated = True\n",
    "            reward = 100\n",
    "        else:\n",
    "            # incentive to keep moving; reach end quickly\n",
    "            reward = -1/(self.dim_row_col*self.dim_row_col)\n",
    "            \n",
    "        # end-condition w/ out reward is too many steps taken\n",
    "        if (self.current_step >= self.max_steps):\n",
    "            self.episode_terminated = True\n",
    "            \n",
    "        # info {} must be returned for step() for Env class; dk why\n",
    "        info = {}\n",
    "        \n",
    "        # return step information\n",
    "        return np.array(self.state, dtype=np.int64), reward, self.episode_terminated, info\n",
    "                \n",
    "    # implements visuals of learning process\n",
    "    def render(self, mode):\n",
    "        # implement viz\n",
    "        clock = pygame.time.Clock()\n",
    "        clock.tick(FPS)\n",
    "        draw_window(self.state, self.past_state)#self.past_past_state\n",
    "        \n",
    "        for event in pygame.event.get():\n",
    "        #Are any of the inputs the X in the top left\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "    \n",
    "    # reset sets env's params to starting values\n",
    "    def reset(self):\n",
    "        # reset state to starting cell\n",
    "        self.state = np.array(self.starting_cell, dtype=np.int64)\n",
    "        # reset past state to starting cell\n",
    "        self.past_state = np.array(self.starting_cell, dtype=np.int64)\n",
    "        # reset episode_terminated to episode running\n",
    "        self.episode_terminated = False\n",
    "        # reset current step to no steps taken\n",
    "        self.current_step = 0\n",
    "        # reset the whole window\n",
    "        WIN.fill(WHITE)\n",
    "        # return state to exploit; model.predict(env.reset()) in test model\n",
    "        #return np.array(self.state, dtype=np.int64)\n",
    "        return self.state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca7c434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dimensions of square maze & starting cell in maze\n",
    "DIM_COL_ROW = 6\n",
    "STARTING_CELL = (0,0)\n",
    "# walls is subject to dim; hardcode changes on input\n",
    "# even x indices are vert walls; up & down actions permittable if val = 0\n",
    "# odd x indices are horz walls; right and left actions permittable if val = 0\n",
    "# end-points of x indices are the edges of the grid\n",
    "# val = 2 => noise in wall grid (wall N/A; neither T/F)\n",
    "# specifically, val = 2 => vert & edges-grid top of grid\n",
    "# for Walls, each row is a set of edges-grid + vert + horz  borders\n",
    "# for Walls, num_col = dim_col_row+1; border numbers\n",
    "WALLS = np.array([\n",
    "                [1,1,1,1,1,1,2],\n",
    "                [1,0,0,0,0,0,1],\n",
    "                [1,0,1,0,1,0,2],\n",
    "                [1,1,1,1,0,1,1],\n",
    "                [0,0,0,0,0,0,2],\n",
    "                [1,0,1,1,1,1,1],\n",
    "                [1,1,0,0,1,0,2],\n",
    "                [1,0,0,0,0,0,1],\n",
    "                [1,1,1,0,1,1,2],\n",
    "                [1,0,0,0,1,1,1],\n",
    "                [0,1,1,1,0,0,2],\n",
    "                [1,0,0,0,0,0,1],\n",
    "                [1,1,1,1,1,1,2]\n",
    "                ])\n",
    "\n",
    "# declare environment\n",
    "env = MazeEnv(DIM_COL_ROW, STARTING_CELL, WALLS)\n",
    "# ensure env functions with stable_baselines well\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17607857",
   "metadata": {},
   "source": [
    "# Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes = 50\n",
    "# average_score = 0\n",
    "# average_steps = 0\n",
    "# # iterate through simulated episodes of the env with random actions\n",
    "# for episode in range(1, episodes+1):\n",
    "#     # make state at starting cell\n",
    "#     state = env.reset()\n",
    "#     # reset boolean for simulation\n",
    "#     done = False\n",
    "#     # score is metric for rewards\n",
    "#     score = 0 \n",
    "#     # logic for scores\n",
    "#     # simulate a singular episode with random action\n",
    "#     while not done:\n",
    "#         # render env\n",
    "#         env.render()\n",
    "#         # random actions hence .sample()\n",
    "#         action = env.action_space.sample()\n",
    "#         # step from state with action & capture step info\n",
    "#         n_state, reward, done, info = env.step(action)\n",
    "#         # increment reward metric\n",
    "#         score+=reward\n",
    "#     # display simulation results\n",
    "#     print('Episode:{} Score:{} Steps:{}'.format(episode, score, int((100-score)*(DIM_COL_ROW*DIM_COL_ROW))))\n",
    "#     # update average_score & average_steps\n",
    "#     average_score += score\n",
    "#     average_steps += int((100-score)*(DIM_COL_ROW*DIM_COL_ROW))\n",
    "# # display simulation average score & steps\n",
    "# print(\"--------------------------------------------------\")\n",
    "# print(\"Average Score:{} Average Steps:{}\".format(average_score/episodes, average_steps/episodes))\n",
    "# # env.close() - needed when render implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f08715",
   "metadata": {},
   "source": [
    "## Train an RL Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c2b8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the paths for saving the model & logging its training statistics\n",
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "log_path = os.path.join('Training', 'Logs')\n",
    "training_log_path = os.path.join(log_path, 'DQN_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0edd50b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# create the model object with MlpPolicy & DQN type\n",
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a129ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.learn(total_timesteps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a21644db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create path for dqn model to be saved & loaded from\n",
    "dqn_path = os.path.join('Training', 'Saved Models', 'DQN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0756dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to dqn path\n",
    "model.save(dqn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f3c9815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# load model from dqn_path including the existing env\n",
    "model = DQN.load(dqn_path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b534a",
   "metadata": {},
   "source": [
    "## Test DQN_Model on MazeEnv - See Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e83a355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs:  [0 0]\n",
      "obs:  [0 1]\n",
      "obs:  [0 2]\n",
      "obs:  [0 3]\n",
      "obs:  [1 3]\n",
      "obs:  [2 3]\n",
      "obs:  [3 3]\n",
      "obs:  [4 3]\n",
      "obs:  [4 2]\n",
      "obs:  [4 1]\n",
      "obs:  [4 0]\n",
      "obs:  [5 0]\n",
      "obs:  [5 1]\n",
      "obs:  [5 2]\n",
      "obs:  [5 3]\n",
      "obs:  [5 4]\n",
      "obs:  [5 5]\n"
     ]
    }
   ],
   "source": [
    "# reset env, run agent predictions until step function indicates ending cell reached\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    print(\"obs: \", obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render(True)\n",
    "    if done: \n",
    "        print('obs: ', obs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc03855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:\\Installs\\Anaconda_Location\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99.58333333022892, 0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run policy on env for n_eval_episodes & return mean reward per episode\n",
    "evaluate_policy(model, env, n_eval_episodes=1000, render=True, reward_threshold = 70)\n",
    "# note the \"Monitor wrapper\" error below has undetermined origins as loading the dqn model auto-wraps the model into a monitor wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0562836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this line to quit popup without crashing kernel\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66bab98fb05b5ac4a4515b72f66cfb10345e282e1f09ccb74aec644d67d52634"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
